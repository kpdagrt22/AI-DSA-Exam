{"cells":[{"source":"# Practical Exam: Customer Purchase Prediction\n\nRetailTech Solutions is a fast-growing international e-commerce platform operating in over 20 countries across Europe, North America, and Asia. They specialize in fashion, electronics, and home goods, with a unique business model that combines traditional retail with a marketplace for independent sellers.\n\nThe company has seen rapid growth. A key part of their success has been their data-driven approach to personalization. However, as they plan their expansion into new markets, they need to improve their ability to predict customer behavior.\n\nTheir marketing team wants to predict which customers are most likely to make a purchase based on their browsing behavior.\n\nAs an AI Engineer, you will help build this prediction system. Your work will directly impact RetailTech's growth strategy and their goal of increasing revenue.\n\n\n## Data Description\n\n| Column Name | Criteria |\n|------------|----------|\n| customer_id | Integer. Unique identifier for each customer. No missing values. |\n| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |","metadata":{},"id":"8d0bcede-0826-475c-8678-72835c042b37","cell_type":"markdown"},{"source":"# Task 1\n\nThe marketing team has collected customer session data in `raw_customer_data.csv`, but it contains missing values and inconsistencies that need to be addressed.\nCreate a cleaned version of the dataframe:\n\n- Start with the data in the file `raw_customer_data.csv`\n- Your output should be a DataFrame named `clean_data`\n- All column names and values should match the table below.\n</br>\n\n| Column Name | Criteria |\n|------------|----------|\n| customer_id | Integer. Unique identifier for each customer. No missing values. |\n| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |","metadata":{},"id":"c0d5a3bb-bbae-4d39-a6c6-daa46c470347","cell_type":"markdown"},{"source":"import pandas as pd\n\n# Load raw customer session data\ndf = pd.read_csv('raw_customer_data.csv')\n\n# Clean and standardize the data\nclean_data = df.copy()\n\n# Fill missing values\nclean_data['time_spent'].fillna(clean_data['time_spent'].median(), inplace=True)\nclean_data['pages_viewed'].fillna(clean_data['pages_viewed'].mean(), inplace=True)\nclean_data['basket_value'].fillna(0.0, inplace=True)\nclean_data['device_type'].fillna('Unknown', inplace=True)\nclean_data['customer_type'].fillna('New', inplace=True)\n\n# Ensure correct data types\nclean_data['customer_id'] = clean_data['customer_id'].astype(int)\nclean_data['time_spent'] = clean_data['time_spent'].astype(float)\nclean_data['pages_viewed'] = clean_data['pages_viewed'].round().astype(int)\nclean_data['basket_value'] = clean_data['basket_value'].astype(float)\nclean_data['purchase'] = clean_data['purchase'].astype(int)\n\n# Preview to verify\nprint(\" Cleaned data sample:\")\nprint(clean_data.head())\nprint(\"\\n Data types:\")\nprint(clean_data.dtypes)\n","metadata":{"executionCancelledAt":null,"executionTime":31,"lastExecutedAt":1749389910668,"lastExecutedByKernel":"185c8caf-c153-421e-8bd2-1af169e578e0","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\n\n# Load raw customer session data\ndf = pd.read_csv('raw_customer_data.csv')\n\n# Clean and standardize the data\nclean_data = df.copy()\n\n# Fill missing values\nclean_data['time_spent'].fillna(clean_data['time_spent'].median(), inplace=True)\nclean_data['pages_viewed'].fillna(clean_data['pages_viewed'].mean(), inplace=True)\nclean_data['basket_value'].fillna(0.0, inplace=True)\nclean_data['device_type'].fillna('Unknown', inplace=True)\nclean_data['customer_type'].fillna('New', inplace=True)\n\n# Ensure correct data types\nclean_data['customer_id'] = clean_data['customer_id'].astype(int)\nclean_data['time_spent'] = clean_data['time_spent'].astype(float)\nclean_data['pages_viewed'] = clean_data['pages_viewed'].round().astype(int)\nclean_data['basket_value'] = clean_data['basket_value'].astype(float)\nclean_data['purchase'] = clean_data['purchase'].astype(int)\n\n# Preview to verify\nprint(\" Cleaned data sample:\")\nprint(clean_data.head())\nprint(\"\\n Data types:\")\nprint(clean_data.dtypes)\n","outputsMetadata":{"0":{"height":563,"type":"stream"}}},"id":"5ce18b54-29af-4beb-bc8c-79c4e21bcd52","cell_type":"code","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":" Cleaned data sample:\n   customer_id  time_spent  pages_viewed  ...  device_type customer_type purchase\n0            1   23.097867             7  ...       Mobile     Returning        0\n1            2   57.092144             3  ...       Mobile     Returning        1\n2            3   44.187643            14  ...       Mobile     Returning        0\n3            4   36.320851            10  ...       Mobile           New        1\n4            5   10.205100            16  ...       Mobile     Returning        1\n\n[5 rows x 7 columns]\n\n Data types:\ncustomer_id        int64\ntime_spent       float64\npages_viewed       int64\nbasket_value     float64\ndevice_type       object\ncustomer_type     object\npurchase           int64\ndtype: object\n"}]},{"source":"# Task 2\nThe pre-cleaned dataset `model_data.csv` needs to be prepared for our neural network.\nCreate the model features:\n\n- Start with the data in the file `model_data.csv`\n- Scale numerical features (`time_spent`, `pages_viewed`, `basket_value`) to 0-1 range\n- Apply one-hot encoding to the categorical features (`device_type`, `customer_type`)\n    - The column names should have the following format: variable_name_category_name (e.g., `device_type_Desktop`)\n- Your output should be a DataFrame named `model_feature_set`, with all column names from `model_data.csv` except for the columns where one-hot encoding was applied.\n","metadata":{},"id":"026b3c30-d3b0-4762-ae10-0f2880873bdc","cell_type":"markdown"},{"source":"# Write your answer to Task 2 here\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Load the dataset\ndf_model = pd.read_csv('model_data.csv')\n\n# Define numerical and categorical features\nnumerical_features = ['time_spent', 'pages_viewed', 'basket_value']\ncategorical_features = ['device_type', 'customer_type']\n\n# Create a preprocessor for scaling numerical features and one-hot encoding categorical features\n# The remainder='passthrough' will keep other columns (like customer_id and purchase)\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', MinMaxScaler(), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# Apply the preprocessing\nX_processed = preprocessor.fit_transform(df_model)\n\n# Get feature names after one-hot encoding\n# The column names for numerical features will be the same\n# The column names for one-hot encoded features need to be retrieved from the OneHotEncoder\nnumerical_feature_names = numerical_features\ncategorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n\n# Get the names of the columns that passed through (customer_id, purchase)\npassthrough_columns = [col for col in df_model.columns if col not in numerical_features + categorical_features]\n\n# Combine all feature names in the correct order\nall_feature_names = list(numerical_feature_names) + list(categorical_feature_names) + list(passthrough_columns)\n\n# Create the final DataFrame\nmodel_feature_set = pd.DataFrame(X_processed, columns=all_feature_names)\n\n# Ensure the columns are in a logical order if necessary, for this task, the order from all_feature_names is fine.\n# Display the first few rows of the model_feature_set and its info to verify\nprint(\"Model Feature Set (First 5 Rows):\")\nprint(model_feature_set.head())\nprint(\"\\nModel Feature Set Info:\")\nprint(model_feature_set.info())","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1749389910724,"lastExecutedByKernel":"185c8caf-c153-421e-8bd2-1af169e578e0","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Write your answer to Task 2 here\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Load the dataset\ndf_model = pd.read_csv('model_data.csv')\n\n# Define numerical and categorical features\nnumerical_features = ['time_spent', 'pages_viewed', 'basket_value']\ncategorical_features = ['device_type', 'customer_type']\n\n# Create a preprocessor for scaling numerical features and one-hot encoding categorical features\n# The remainder='passthrough' will keep other columns (like customer_id and purchase)\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', MinMaxScaler(), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# Apply the preprocessing\nX_processed = preprocessor.fit_transform(df_model)\n\n# Get feature names after one-hot encoding\n# The column names for numerical features will be the same\n# The column names for one-hot encoded features need to be retrieved from the OneHotEncoder\nnumerical_feature_names = numerical_features\ncategorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n\n# Get the names of the columns that passed through (customer_id, purchase)\npassthrough_columns = [col for col in df_model.columns if col not in numerical_features + categorical_features]\n\n# Combine all feature names in the correct order\nall_feature_names = list(numerical_feature_names) + list(categorical_feature_names) + list(passthrough_columns)\n\n# Create the final DataFrame\nmodel_feature_set = pd.DataFrame(X_processed, columns=all_feature_names)\n\n# Ensure the columns are in a logical order if necessary, for this task, the order from all_feature_names is fine.\n# Display the first few rows of the model_feature_set and its info to verify\nprint(\"Model Feature Set (First 5 Rows):\")\nprint(model_feature_set.head())\nprint(\"\\nModel Feature Set Info:\")\nprint(model_feature_set.info())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"6d47e440-c4ab-45cf-af40-53181764bac4","cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"Model Feature Set (First 5 Rows):\n   time_spent  pages_viewed  ...  customer_id  purchase\n0    0.664167      0.500000  ...        501.0       1.0\n1    0.483681      0.222222  ...        502.0       1.0\n2    0.231359      0.111111  ...        503.0       0.0\n3    0.792944      0.277778  ...        504.0       1.0\n4    0.649210      0.166667  ...        505.0       1.0\n\n[5 rows x 11 columns]\n\nModel Feature Set Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 11 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   time_spent               500 non-null    float64\n 1   pages_viewed             500 non-null    float64\n 2   basket_value             500 non-null    float64\n 3   device_type_Desktop      500 non-null    float64\n 4   device_type_Mobile       500 non-null    float64\n 5   device_type_Tablet       500 non-null    float64\n 6   device_type_Unknown      500 non-null    float64\n 7   customer_type_New        500 non-null    float64\n 8   customer_type_Returning  500 non-null    float64\n 9   customer_id              500 non-null    float64\n 10  purchase                 500 non-null    float64\ndtypes: float64(11)\nmemory usage: 43.1 KB\nNone\n"}]},{"source":"# Task 3\n\nNow that all preparatory work has been done, create and train a neural network that would allow the company to predict purchases.\n\n- Using PyTorch, create a network with:\n   - At least one hidden layer with 8 units\n   - ReLU activation for hidden layer\n   - Sigmoid activation for the output layer\n- Using the prepared features in `input_model_features.csv`, train the model to predict purchases. \n- Use the validation dataset `validation_features.csv` to predict new values based on the trained model. \n- Your model should be named `purchase_model` and your output should be a DataFrame named `validation_predictions` with columns `customer_id` and `purchase`. The `purchase` column must be your predicted values.\n","metadata":{},"id":"10a02327-d528-441c-87bf-098f9d6415e1","cell_type":"markdown"},{"source":"# Write your answer to Task 3 here\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load the datasets\ninput_model_features = pd.read_csv('input_model_features.csv')\nvalidation_features = pd.read_csv('validation_features.csv')\n\n# Separate features and target for training data\n# 'purchase' is the target variable\nX_train_df = input_model_features.drop(columns=['customer_id', 'purchase'])\ny_train_df = input_model_features['purchase']\n\n# Convert to PyTorch tensors\nX_train = torch.tensor(X_train_df.values, dtype=torch.float32)\ny_train = torch.tensor(y_train_df.values, dtype=torch.float32).view(-1, 1) # Reshape for BCELoss\n\n# For validation data, store customer_id separately and drop 'purchase' if it exists (it shouldn't based on task)\ncustomer_ids_validation = validation_features['customer_id']\nX_validation_df = validation_features.drop(columns=['customer_id', 'purchase'], errors='ignore') # 'purchase' might not be in validation_features for prediction\nX_validation = torch.tensor(X_validation_df.values, dtype=torch.float32)\n\n# Define the neural network model\nclass PurchasePredictor(nn.Module):\n    def __init__(self, input_size):\n        super(PurchasePredictor, self).__init__()\n        # At least one hidden layer with 8 units and ReLU activation\n        self.fc1 = nn.Linear(input_size, 8)\n        self.relu = nn.ReLU()\n        # Output layer with Sigmoid activation for binary classification\n        self.fc2 = nn.Linear(8, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# Instantiate the model\ninput_size = X_train.shape[1]\npurchase_model = PurchasePredictor(input_size)\n\n# Define loss function and optimizer\ncriterion = nn.BCELoss() # Binary Cross-Entropy Loss\noptimizer = optim.Adam(purchase_model.parameters(), lr=0.001)\n\n# Create DataLoader for training\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Train the model\nnum_epochs = 100 # A reasonable number of epochs for a simple model\nfor epoch in range(num_epochs):\n    purchase_model.train() # Set model to training mode\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = purchase_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n# Predict on the validation dataset\npurchase_model.eval() # Set model to evaluation mode\nwith torch.no_grad(): # Disable gradient calculation for prediction\n    y_pred_proba_validation = purchase_model(X_validation)\n    # Convert probabilities to binary predictions (0 or 1)\n    y_pred_validation = (y_pred_proba_validation >= 0.5).int()\n\n# Create the validation_predictions DataFrame\nvalidation_predictions = pd.DataFrame({\n    'customer_id': customer_ids_validation,\n    'purchase': y_pred_validation.numpy().flatten()\n})\n\nprint(\"Validation Predictions (First 5 Rows):\")\nprint(validation_predictions.head())\nprint(\"\\nValidation Predictions Info:\")\nprint(validation_predictions.info())","metadata":{"executionCancelledAt":null,"executionTime":7708,"lastExecutedAt":1749389918433,"lastExecutedByKernel":"185c8caf-c153-421e-8bd2-1af169e578e0","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Write your answer to Task 3 here\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load the datasets\ninput_model_features = pd.read_csv('input_model_features.csv')\nvalidation_features = pd.read_csv('validation_features.csv')\n\n# Separate features and target for training data\n# 'purchase' is the target variable\nX_train_df = input_model_features.drop(columns=['customer_id', 'purchase'])\ny_train_df = input_model_features['purchase']\n\n# Convert to PyTorch tensors\nX_train = torch.tensor(X_train_df.values, dtype=torch.float32)\ny_train = torch.tensor(y_train_df.values, dtype=torch.float32).view(-1, 1) # Reshape for BCELoss\n\n# For validation data, store customer_id separately and drop 'purchase' if it exists (it shouldn't based on task)\ncustomer_ids_validation = validation_features['customer_id']\nX_validation_df = validation_features.drop(columns=['customer_id', 'purchase'], errors='ignore') # 'purchase' might not be in validation_features for prediction\nX_validation = torch.tensor(X_validation_df.values, dtype=torch.float32)\n\n# Define the neural network model\nclass PurchasePredictor(nn.Module):\n    def __init__(self, input_size):\n        super(PurchasePredictor, self).__init__()\n        # At least one hidden layer with 8 units and ReLU activation\n        self.fc1 = nn.Linear(input_size, 8)\n        self.relu = nn.ReLU()\n        # Output layer with Sigmoid activation for binary classification\n        self.fc2 = nn.Linear(8, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n# Instantiate the model\ninput_size = X_train.shape[1]\npurchase_model = PurchasePredictor(input_size)\n\n# Define loss function and optimizer\ncriterion = nn.BCELoss() # Binary Cross-Entropy Loss\noptimizer = optim.Adam(purchase_model.parameters(), lr=0.001)\n\n# Create DataLoader for training\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Train the model\nnum_epochs = 100 # A reasonable number of epochs for a simple model\nfor epoch in range(num_epochs):\n    purchase_model.train() # Set model to training mode\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = purchase_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n# Predict on the validation dataset\npurchase_model.eval() # Set model to evaluation mode\nwith torch.no_grad(): # Disable gradient calculation for prediction\n    y_pred_proba_validation = purchase_model(X_validation)\n    # Convert probabilities to binary predictions (0 or 1)\n    y_pred_validation = (y_pred_proba_validation >= 0.5).int()\n\n# Create the validation_predictions DataFrame\nvalidation_predictions = pd.DataFrame({\n    'customer_id': customer_ids_validation,\n    'purchase': y_pred_validation.numpy().flatten()\n})\n\nprint(\"Validation Predictions (First 5 Rows):\")\nprint(validation_predictions.head())\nprint(\"\\nValidation Predictions Info:\")\nprint(validation_predictions.info())","outputsMetadata":{"0":{"height":416,"type":"stream"}}},"id":"efcbda28-3c89-480d-b77a-c7f27ac759d5","cell_type":"code","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"Validation Predictions (First 5 Rows):\n   customer_id  purchase\n0         1801         1\n1         1802         1\n2         1803         1\n3         1804         1\n4         1805         1\n\nValidation Predictions Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype\n---  ------       --------------  -----\n 0   customer_id  200 non-null    int64\n 1   purchase     200 non-null    int32\ndtypes: int32(1), int64(1)\nmemory usage: 2.5 KB\nNone\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}